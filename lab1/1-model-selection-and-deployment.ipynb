{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection and Deployment with Prompt Engineering\n",
    "\n",
    "---\n",
    "\n",
    "This workshop uses SageMaker Notebook, and please ensure the kernel is set to **conda_python3**.\n",
    "\n",
    "In this notebook we will demonstrate:\n",
    "\n",
    "1. How to deploy Large Language Models (LLMs) in SageMaker Jumpstart; \n",
    "\n",
    "2. Common use cases of LLMs in the post call scenario;\n",
    "\n",
    "3. Ask a question to LLMs with or without providing the examples. \n",
    "\n",
    "### Contents\n",
    "\n",
    "- [1. Deploy Large Language Models (LLMs) in SageMaker JumpStart](#1.-Deploy-Large-Language-Models-in-SageMaker-JumpStart)\n",
    "- [2. Common Use Cases of Language Models](#2.-Common-Use-Cases-of-Language-Models)\n",
    "- [3. Ask Your Questions to Language Models](#3.-Ask-Your-Questions-to-Language-Models)\n",
    "- [4. Delete the Endpoint](#4.-Delete-the-Endpoint)\n",
    "\n",
    "**Note**\n",
    "* This notebook serves as a template so that you can easily replace the example dataset with your own to build a custom question and answering application.\n",
    "* This lab will take you 25 mins with 15 mins for deployment and 10 mins for testing models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Deploy Large Language Models in SageMaker JumpStart\n",
    "\n",
    "---\n",
    "\n",
    "Let's first deploy all the models required to perform the performance comparison. \n",
    "\n",
    "In the workshop, we will deploy **FLAN T5 Small** and **FLAN T5 XL** and compare them. Please refer to the information below for the model sizes of the FLAN T5 family.\n",
    "\n",
    "* FLAN T5 Small (60M)\n",
    "* FLAN T5 Base (250M)\n",
    "* FLAN T5 Large (780M)\n",
    "* FLAN T5 XL (3B)\n",
    "* FLAN T5 XXL (11B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "model_version = \"*\"\n",
    "\n",
    "sagemaker = boto3.client('sagemaker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint_with_json_payload(encoded_json, endpoint_name, content_type=\"application/json\"):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=content_type, Body=encoded_json\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_model_flan_t5(query_response):\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text\n",
    "\n",
    "\n",
    "def parse_response_model_bloomz(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    generated_text = model_predictions[\"generated_texts\"]\n",
    "    return generated_text"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can deploy additional models on your own to compare their performance. To achieve this, you'll need to make adjustments to the `_MODEL_CONFIG_` dictionary as outlined below. \n",
    "\n",
    "**Note**\n",
    "\n",
    "* You may check [avaliable models](https://sagemaker.readthedocs.io/en/stable/doc_utils/pretrainedmodels.html) on Amazon SageMaker JumpStart to get the avaliable mode list, and [g5 pricing](https://aws.amazon.com/ec2/instance-types/g5/) to estimate the budget.\n",
    "\n",
    "* Please note that if you use up all the GPU instance quotas, you may need to delete the previous endpoints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_MODEL_CONFIG_ = {\n",
    "    \"huggingface-text2text-flan-t5-small\": {\n",
    "        \"instance type\": \"ml.g5.xlarge\",\n",
    "        \"env\": {},\n",
    "        \"parse_function\": parse_response_model_flan_t5,\n",
    "        \"prompt\": \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\",\n",
    "    },\n",
    "    \"huggingface-text2text-flan-t5-xl\": {\n",
    "        \"instance type\": \"ml.g5.2xlarge\",\n",
    "        \"env\": {},\n",
    "        \"parse_function\": parse_response_model_flan_t5,\n",
    "        \"prompt\": \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\",        \n",
    "    },\n",
    "    \"huggingface-textgeneration-bloomz-1b7\": {\n",
    "        \"instance type\": \"ml.p3.2xlarge\",\n",
    "        \"env\": {},\n",
    "        \"parse_function\": parse_response_model_bloomz,\n",
    "        \"prompt\": \"\"\"Answer based on context:\\n\\n{context}\\n\\n{question}\"\"\",            \n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's deploy models to the endpoints.\n",
    "\n",
    "The below cell will run for about 15 mins as you deploy three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "newline, bold, unbold = \"\\n\", \"\\033[1m\", \"\\033[0m\"\n",
    "\n",
    "endpoints = sagemaker.list_endpoints()['Endpoints']\n",
    "\n",
    "if not endpoints:\n",
    "    # Deploy models to the endpoints\n",
    "    for model_id in _MODEL_CONFIG_.keys():\n",
    "\n",
    "        print(\"Model ID: \",  model_id)\n",
    "        endpoint_name = name_from_base(f\"jp-{model_id}\", short=False)\n",
    "        inference_instance_type = _MODEL_CONFIG_[model_id][\"instance type\"]\n",
    "\n",
    "        print(\"Endpoint Name: \", endpoint_name)\n",
    "        # Retrieve the inference container uri. This is the base HuggingFace container image for the default model above.\n",
    "        deploy_image_uri = image_uris.retrieve(\n",
    "            region=None,\n",
    "            framework=None,  # automatically inferred from model_id\n",
    "            image_scope=\"inference\",\n",
    "            model_id=model_id,\n",
    "            model_version=model_version,\n",
    "            instance_type=inference_instance_type,\n",
    "        )\n",
    "        # Retrieve the model uri.\n",
    "        model_uri = model_uris.retrieve(\n",
    "            model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    "        )\n",
    "\n",
    "        model_inference = Model(\n",
    "            image_uri=deploy_image_uri,\n",
    "            model_data=model_uri,\n",
    "            role=aws_role,\n",
    "            predictor_cls=Predictor,\n",
    "            name=endpoint_name,\n",
    "            env=_MODEL_CONFIG_[model_id][\"env\"],\n",
    "        )\n",
    "        model_predictor_inference = model_inference.deploy(\n",
    "            initial_instance_count=1,\n",
    "            instance_type=inference_instance_type,\n",
    "            predictor_cls=Predictor,\n",
    "            endpoint_name=endpoint_name,\n",
    "        )\n",
    "\n",
    "        print(f\"{bold}Model {model_id} has been deployed successfully.{unbold}{newline}\")\n",
    "        _MODEL_CONFIG_[model_id][\"endpoint_name\"] = endpoint_name\n",
    "        print(\"---\")\n",
    "\n",
    "else:\n",
    "    # Reuse deployed endpoints\n",
    "    for endpoint in endpoints:\n",
    "        endpoint_name = endpoint['EndpointName']\n",
    "        print(endpoint_name)\n",
    "        \n",
    "        for model_id in _MODEL_CONFIG_.keys():\n",
    "            if model_id[:-1] in endpoint_name:\n",
    "                _MODEL_CONFIG_[model_id][\"endpoint_name\"] = endpoint_name\n",
    "        \n",
    "    print(\"---\")   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Common Use Cases of Language Models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Text summarization\n",
    "- Common sense reasoning\n",
    "- Question answering\n",
    "- Sentiment classification\n",
    "- Translation\n",
    "- Pronoun resolution (代名詞解析)\n",
    "- Text generation based on articles\n",
    "- Imaginary article based on the title\n",
    "\n",
    "Here are more sample queries for your reference: [Zero-shot prompting for the Flan-T5 foundation model in Amazon SageMaker JumpStart](https://aws.amazon.com/blogs/machine-learning/zero-shot-prompting-for-the-flan-t5-foundation-model-in-amazon-sagemaker-jumpstart/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the results from different LLMs\n",
    "\n",
    "Now, let's test the three models - Flan T5 Small, Flan T5 XL, and BLOOMZ - with three sample prompts in the post call scenario. You can check the `transcripts` folder for the raw data of all call transcripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample transcript 1\n",
    "f = open(\"transcripts/neutral-short.txt\", \"r\")\n",
    "transcript_account = f.read()\n",
    "\n",
    "# sample transcript 2\n",
    "f = open(\"transcripts/negative-refund.txt\", \"r\")\n",
    "transcript_neg_refund = f.read()\n",
    "\n",
    "# sample transcript 3\n",
    "f = open(\"transcripts/positive-partial-refund.txt\", \"r\")\n",
    "transcript_pos_refund = f.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we design a prompt template to allow for the replacement of both the transcript and the instruction.\n",
    "\n",
    "```python\n",
    "question = \"\"\"\n",
    "Here is what customer said in the call: \n",
    "{transcript}\n",
    "\n",
    "\n",
    "{objective}\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "Here, we design four prompts for different objectives: (1) identifying call intents, (2)  analyzing sentiment, (3) classifying problem resolution, and (4) summarizing calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purpose 1\n",
    "prompt_intent = \"Based on the transcript, what's the purpose of the call?\"\n",
    "\n",
    "# purpose 2\n",
    "prompt_sentiment = \"Based on the transcript, what's the sentiment of the customer?\"\n",
    "\n",
    "# purpose 3\n",
    "prompt_resolution = \"Based on the transcript, is the customer's need resolved?\"\n",
    "\n",
    "# purpose 4\n",
    "prompt_summary = \"Please summarize the transcript above.\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's merge the transcript and the instruction together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"\"\"\n",
    "Here is what customer said in the call: \n",
    "{transcript}\n",
    "    \n",
    "\n",
    "{objective}\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adjusting the hyperparameters of the LLM, you can experiment by modifying the values within the `payload` dictionary in the cell below.\n",
    "\n",
    "* **text_inputs** : This is where your prompt goes;\n",
    "* **max_length** : The model generates text until the output reaches the specified maximum length;\n",
    "* **num_return_sequences** : This determines the number of output sequences returned;\n",
    "* **top_k** : At each step of text generation, the model samples from the top k most likely words;\n",
    "* **top_p** : At each step of text generation, the model samples from the smallest possible set of words with cumulative probability top p;\n",
    "* **do_sample** : If set to True, the model samples the next word based on its likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : replace \"transcript\" and \"objective\"\n",
    "prompt = question.format(transcript=transcript_pos_refund, objective=prompt_intent)\n",
    "\n",
    "payload = {\n",
    "    \"text_inputs\": prompt,\n",
    "    \"max_length\": 500,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": 3,\n",
    "    \"top_k\": 10,\n",
    "    \"top_p\": 0.95,\n",
    "    \"do_sample\": True,\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's send the prompt to all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_id in _MODEL_CONFIG_:\n",
    "    endpoint_name = _MODEL_CONFIG_[model_id][\"endpoint_name\"]\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = _MODEL_CONFIG_[model_id][\"parse_function\"](query_response)\n",
    "    print(f\"For model: {model_id}, the generated output is: {generated_texts[0]}\\n\")\n",
    "\n",
    "print(\"---\\nHere goes the prompt:\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Ask Your Questions to Language Models\n",
    "\n",
    "---\n",
    "\n",
    "Experiment with your own prompts and observe the responses you receive from all models. Explore zero-shot and few-shot prompting strategies to assess the performance of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : set \"WITH_TAG_INFO\" to be \"True\"\n",
    "WITH_TAG_INFO = False # True\n",
    "\n",
    "if WITH_TAG_INFO:\n",
    "    prompt = \"\"\"\n",
    "    This is a sentiment analysis. Please choose a tag provided below as your answer:\n",
    "    POSiTIVE\n",
    "    NEuTRAL\n",
    "    NEGaTIVE\n",
    "\n",
    "    What's the sentiment of the below statement:\n",
    "\n",
    "    The weather conditions are expected to remain stable with clear skies and gentle breezes.\n",
    "\n",
    "    Answer:\n",
    "    \"\"\"\n",
    "else: \n",
    "    prompt = \"\"\"\n",
    "    This is a sentiment analysis. \n",
    "\n",
    "    What's the sentiment of the below statement:\n",
    "\n",
    "    The weather conditions are expected to remain stable with clear skies and gentle breezes.\n",
    "\n",
    "    Answer:\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"text_inputs\": prompt,\n",
    "    \"max_length\": 100,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    \"top_p\": 1.0, #0.95,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in _MODEL_CONFIG_:\n",
    "    endpoint_name = _MODEL_CONFIG_[model_id][\"endpoint_name\"]\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = _MODEL_CONFIG_[model_id][\"parse_function\"](query_response)\n",
    "    print(f\"For model: {model_id}, the generated output is: {generated_texts[0]}\\n\")\n",
    "\n",
    "print(\"---\\nHere goes the prompt:\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More examples\n",
    "\n",
    "Below are some examples you can try:\n",
    "\n",
    "Example 1\n",
    "* Input Text: \"Translate hello in french:\"\n",
    "* Model Prediction: 'generated_text': 'Bonjour'\n",
    "\n",
    "Example 2\n",
    "* Input Text : \"A step by step recipe to make bolognese pasta:\"\n",
    "* Model Prediction: 'generated_text': 'Toss the pasta with the sauce, then add the meat and toss again.'  \n",
    "\n",
    "Example 3\n",
    "* Input Text : \"Who's Blackpink:\"\n",
    "* Model Prediction: 'generated_text': 'Kpop girl group' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with the given example or your own prompt\n",
    "prompt = \"Who's Blackpink:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"text_inputs\": prompt,\n",
    "    \"max_length\": 100,\n",
    "    \"max_time\": 50,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 1,\n",
    "    \"top_p\": 1.0, #0.95,\n",
    "    \"do_sample\": False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_id in _MODEL_CONFIG_:\n",
    "    endpoint_name = _MODEL_CONFIG_[model_id][\"endpoint_name\"]\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = _MODEL_CONFIG_[model_id][\"parse_function\"](query_response)\n",
    "    print(f\"For model: {model_id}, the generated output is: {generated_texts[0]}\\n\")\n",
    "\n",
    "print(\"---\\nHere goes the prompt:\\n\")\n",
    "print(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Delete the Endpoint\n",
    "---\n",
    "- Delete: huggingface-text2text-flan-t5-xl\n",
    "- Delete: huggingface-text2text-flan-t5-small\n",
    "- Delete: huggingface-textgeneration-bloomz-1b7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_MODEL_CONFIG_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace with the endpoint name\n",
    "endpoint_name = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if endpoint_name is not None:\n",
    "    sagemaker.delete_endpoint(EndpointName=endpoint_name)\n",
    "else:\n",
    "    print(\"Please provide the endpoint name for deletion.\")"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
